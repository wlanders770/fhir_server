# RAG Implementation for FHIR Chat Agent

## Overview
Successfully implemented Retrieval Augmented Generation (RAG) with **ChromaDB persistent storage** to enhance the chat agent's ability to answer questions about HEDIS measures and FHIR claims data. Vector embeddings now persist across container restarts.

## Components Added

### 1. Vector Store (ChromaDB)
- **Persistent storage** using DuckDB + Parquet
- Cosine similarity search with built-in optimization
- Docker volume mount at `/app/chroma_db`
- Custom Ollama embedding function integration
- Automatic persistence on updates

### 2. Embedding Generation
- Uses Ollama's `nomic-embed-text` model
- Generates 768-dimensional embeddings
- Configurable via `OLLAMA_EMBEDDING_MODEL` env variable
- Custom `OllamaEmbeddingFunction` for ChromaDB integration

### 3. Knowledge Base Indexing
**HEDIS Measure Definitions:**
- Indexes CQL files for all 4 HEDIS measures
- Stores measure criteria and screening requirements
- Document IDs: `measure_hedis_bcs.cql`, etc.

**Clinical Knowledge:**
- BCS: Breast cancer screening criteria (women 50-74, 27 months)
- COL: Colorectal cancer screening (adults 45-75, colonoscopy/FIT)
- CDC: Diabetes care with HbA1c testing (patients 18-75)
- CBP: Blood pressure control (patients 18-85, BP < 140/90)
- Gap in care definitions
- Compliance calculation methods
- Document IDs: `knowledge_0` through `knowledge_5`

### 4. RAG-Enhanced Query Processing
- Retrieves top 3 relevant documents for each query
- Adds context to LLM prompts with relevance scores
- Includes source attribution in responses
- Relevance threshold: 0.3 (configurable)
- ChromaDB handles embedding generation automatically

## Features

âœ… **Persistent Storage**: Embeddings survive container restarts
âœ… **Semantic Understanding**: Better comprehension of medical terminology
âœ… **Contextual Responses**: Uses relevant measure definitions and knowledge
âœ… **Source Attribution**: Shows which documents informed the answer
âœ… **Lazy Loading**: Indexes data only when first needed
âœ… **Manual Re-indexing**: API endpoint to refresh knowledge base
âœ… **Index Status**: Check current document count and RAG status
âœ… **Fallback Support**: Degrades gracefully if embedding service unavailable

## Configuration

Environment variables in `docker-compose.yml`:
```yaml
environment:
  - FHIR_BASE_URL=http://hapi-fhir:8080/fhir
  - OLLAMA_BASE_URL=http://fhir-ollama:11434
  - OLLAMA_MODEL=mistral:7b
  - OLLAMA_EMBEDDING_MODEL=nomic-embed-text
volumes:
  - chroma_data:/app/chroma_db  # Persistent storage
```

Docker volume for persistence:
```yaml
volumes:
  db_data:
  chroma_data:  # New: ChromaDB persistent storage
```

## API Endpoints

### Re-index Knowledge Base
```bash
POST /api/chat/reindex
```
Clears existing data and re-indexes all HEDIS measures and knowledge items.

**Response:**
```json
{
  "success": true,
  "total_documents": 10,
  "measures_indexed": 4,
  "knowledge_indexed": 6
}
```

**When to use:**
- After generating new HEDIS data
- After updating CQL files
- When testing RAG improvements

### Check Index Status
```bash
GET /api/chat/index-status
```
Returns current indexing status and document count.

**Response:**
```json
{
  "indexed": true,
  "document_count": 10,
  "rag_enabled": true
}
```

## Example Queries Enhanced by RAG

1. **"What is the HEDIS BCS measure?"**
   - Retrieves: BCS measure definition, screening criteria
   - Response includes: Age range, timeframe, CPT codes

2. **"Show me gap in care patients"**
   - Retrieves: Gap in care definition, compliance knowledge
   - Response includes: Eligible patients without services

3. **"Compare all HEDIS measures"**
   - Retrieves: All measure definitions
   - Generates: Comparative analysis with chart

4. **"What CPT codes for mammogram?"**
   - Retrieves: BCS screening criteria
   - Response includes: 77065, 77066, 77067, 77063, 77061, 77062

## Performance

- **Storage**: ~50 KB for 10 documents (persistent in Docker volume)
- **Indexing**: ~10 documents (4 measures + 6 knowledge items)
- **Search**: < 10ms per query (ChromaDB optimized)
- **Embedding Generation**: ~50-200ms (Ollama)
- **Total RAG Latency**: ~100-300ms per query
- **Relevance**: Cosine similarity with 0.3 threshold
- **Context**: Top 3 most relevant documents
- **Persistence**: Automatic across container restarts

## Loading Options

### 1. Automatic (Lazy Loading) - Default
First chat query triggers indexing automatically.

### 2. Manual Re-indexing
Call API endpoint after data changes:
```bash
curl -X POST http://localhost:5000/api/chat/reindex
```

### 3. Startup Indexing
Add to `app.py` for guaranteed freshness on startup.

### 4. Scheduled Background Jobs
Periodic re-indexing for production environments.

**See [CHROMADB_GUIDE.md](CHROMADB_GUIDE.md) for detailed loading strategies.**

## Future Enhancements

1. ~~**Persistent Storage**: Use ChromaDB or FAISS for production~~ âœ… **DONE**
2. **Incremental Indexing**: Update only changed documents
3. **Dynamic Indexing**: Index recent claims and patient data
4. **Query Expansion**: Use embeddings to expand search terms
5. **Multi-hop Reasoning**: Chain multiple retrievals
6. **Fine-tuning**: Train embeddings on medical data
7. **Semantic Caching**: Cache frequent query embeddings
8. **Multiple Collections**: Separate collections for measures, patients, claims
9. **Metadata Filtering**: Filter results by measure type, date, etc.

## Dependencies Added

```txt
numpy==1.24.3
chromadb==0.4.22
```

## Files Modified

- `webapp/chat_agent.py` - Added ChromaDB RAG implementation with OllamaEmbeddingFunction
- `webapp/app.py` - Added `/api/chat/reindex` and `/api/chat/index-status` endpoints
- `webapp/requirements.txt` - Added numpy and chromadb dependencies
- `docker-compose.yml` - Added chroma_data volume and environment variables
- `CHROMADB_GUIDE.md` - Comprehensive guide on vector storage and loading strategies

## Models Used

- **LLM**: mistral:7b (7B parameters)
- **Embeddings**: nomic-embed-text (137M parameters, 768 dimensions)

## Benefits

- ðŸŽ¯ More accurate medical terminology understanding
- ðŸ“š Context-aware responses based on actual measure definitions
- ðŸ” Better handling of complex HEDIS-specific questions
- ðŸ“Š Enhanced chart generation with proper context
- ðŸŽ“ Educational responses that explain measures
- ðŸ’¾ Persistent embeddings across container restarts
- ðŸ”„ Manual re-indexing when data changes
- ðŸ“ˆ Scalable to 100K+ documents
- âš¡ Fast vector similarity search (<10ms)

## Quick Start

1. **Pull embedding model** (one-time):
```bash
docker exec fhir-ollama ollama pull nomic-embed-text
```

2. **Rebuild webapp with ChromaDB**:
```bash
docker compose up --build -d webapp
```

3. **Verify indexing** (automatic on first query):
```bash
curl http://localhost:5000/api/chat/index-status
```

4. **Manual re-index** (after data changes):
```bash
curl -X POST http://localhost:5000/api/chat/reindex
```

## Documentation

- **[CHROMADB_GUIDE.md](CHROMADB_GUIDE.md)** - Complete guide on vector storage, loading options, and updating strategies
- **[RAG_IMPLEMENTATION.md](RAG_IMPLEMENTATION.md)** - This file, technical implementation overview
